{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from mnist_dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    \n",
    "    def __init__(self, x_size, y_size, z_size):\n",
    "        self.x_size = x_size\n",
    "        self.y_size = y_size\n",
    "        self.z_size = z_size\n",
    "    \n",
    "        self.__build()\n",
    "    \n",
    "    def __build(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.__placeholders()\n",
    "            \n",
    "            # generator\n",
    "            z = tf.random_uniform((tf.shape(self.X_lab)[0], self.z_size))\n",
    "            g_model = self.__get_generator(z)\n",
    "            \n",
    "            # discriminators\n",
    "            d_lab, _ = self.__get_discriminator(self.X_lab)\n",
    "            d_unl, fn_1 = self.__get_discriminator(self.X_unl, True)\n",
    "            d_fake, fn_2 = self.__get_discriminator(g_model, True)\n",
    "            \n",
    "            # losses\n",
    "            ## Discriminator Loss\n",
    "            y_one_hot = tf.one_hot(self.y, 10)\n",
    "            loss_lab = tf.nn.softmax_cross_entropy_with_logits_v2(logits=d_lab, labels=y_one_hot)\n",
    "            loss_lab = tf.reduce_mean(loss_lab)\n",
    "            \n",
    "            l_unl = tf.reduce_logsumexp(d_unl, axis=1)\n",
    "            l_gen = tf.reduce_logsumexp(d_fake, axis=1)\n",
    "            loss_unl = -.5*tf.reduce_mean(l_unl) + \\\n",
    "                        .5*tf.reduce_mean(tf.nn.softplus(l_unl)) + \\\n",
    "                        .5*tf.reduce_mean(tf.nn.softplus(l_gen))\n",
    "            \n",
    "            self.loss_disc = loss_lab + loss_unl\n",
    "            \n",
    "            ## Generator Loss\n",
    "            self.loss_gen = tf.losses.mean_squared_error(fn_1, fn_2)\n",
    "            \n",
    "            # optimizer\n",
    "            t_vars = tf.trainable_variables()\n",
    "            g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "            d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "            \n",
    "            self.opt_disc = tf.train.AdamOptimizer(self.eta, .5).minimize(self.loss_disc, var_list=d_vars)\n",
    "            self.opt_gen  = tf.train.AdamOptimizer(self.eta, .5).minimize(self.loss_gen, var_list=g_vars)\n",
    "        \n",
    "        self.predict = d_lab\n",
    "        self.generator = g_model\n",
    "    \n",
    "    def __placeholders(self):\n",
    "        self.X_lab = tf.placeholder(tf.float32, [None, self.x_size], 'X')\n",
    "        self.X_unl = tf.placeholder(tf.float32, [None, self.x_size], 'X_')\n",
    "        self.y = tf.placeholder(tf.int32, [None, self.y_size], 'y')\n",
    "        \n",
    "        self.eta = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        self.is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    \n",
    "    def __get_generator(self, tensor, reuse=False):\n",
    "        with tf.variable_scope('generator', reuse=reuse):\n",
    "            tensor = tf.layers.dense(tensor, 500, tf.nn.softplus)\n",
    "            tensor = tf.layers.batch_normalization(tensor, training=self.is_training)\n",
    "            \n",
    "            tensor = tf.layers.dense(tensor, 500, tf.nn.softplus)\n",
    "            tensor = tf.layers.batch_normalization(tensor, training=self.is_training)\n",
    "            \n",
    "            tensor = tf.layers.dense(tensor, self.x_size, tf.nn.sigmoid)\n",
    "        return tf.nn.l2_normalize(tensor, axis=1)\n",
    "    \n",
    "    def __get_discriminator(self, tensor, reuse=False):\n",
    "        with tf.variable_scope('discriminator', reuse=reuse):\n",
    "            tensor = self.__add_noise(tensor, .3)\n",
    "            \n",
    "            tensor = tf.layers.dense(tensor, 1000, tf.nn.relu)\n",
    "            tensor = self.__add_noise(tensor, .5)\n",
    "            \n",
    "            tensor = tf.layers.dense(tensor, 500, tf.nn.relu)\n",
    "            tensor = self.__add_noise(tensor, .5)\n",
    "            \n",
    "            tensor = tf.layers.dense(tensor, 250, tf.nn.relu)\n",
    "            tensor = self.__add_noise(tensor, .5)\n",
    "            \n",
    "            tensor = tf.layers.dense(tensor, 250, tf.nn.relu)\n",
    "            tensor = self.__add_noise(tensor, .5)\n",
    "            \n",
    "            tensor = fn = tf.layers.dense(tensor, 250, tf.nn.relu)\n",
    "            tensor = self.__add_noise(tensor, .5)\n",
    "            \n",
    "            tensor = tf.layers.dense(tensor, 10)\n",
    "        return tensor, fn\n",
    "    \n",
    "    def __add_noise(self, tensor, mu=0., std=1.):\n",
    "        noise = lambda: tensor + tf.random_normal(tf.shape(tensor), stddev=std)\n",
    "        no_noise = lambda: tensor\n",
    "        \n",
    "        return tf.cond(self.is_training, noise, no_noise)\n",
    "    \n",
    "model = GAN(784, 1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 1000\n",
    "ix = np.random.choice(range(42000), SAMPLE_SIZE)\n",
    "not_in_ix = [i for i in range(42000) if i not in ix]\n",
    "\n",
    "dataset = Dataset('mnist_dataset/')\n",
    "X = dataset.train['images'][ix]\n",
    "y = dataset.train['labels'][ix]\n",
    "\n",
    "X_un = dataset.train['images'][not_in_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X, y, batch_size):\n",
    "    ix = np.random.choice(len(X), batch_size)\n",
    "    return X[ix], y[ix]\n",
    "\n",
    "def next_batch_(X, batch_size):\n",
    "    ix = np.random.choice(len(X), batch_size)\n",
    "    return X[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25/300 - DLoss: 1.334552 - GLoss: 13.421591\n",
      "Epoch  50/300 - DLoss: 1.018598 - GLoss: 16.411457\n",
      "Epoch  75/300 - DLoss: 0.966072 - GLoss: 25.426922\n",
      "Epoch 100/300 - DLoss: 0.908740 - GLoss: 23.488132\n",
      "Epoch 125/300 - DLoss: 0.824247 - GLoss: 22.732841\n",
      "Epoch 150/300 - DLoss: 0.789415 - GLoss: 25.632410\n",
      "Epoch 175/300 - DLoss: 0.711045 - GLoss: 26.188156\n",
      "Epoch 200/300 - DLoss: 0.664727 - GLoss: 29.538647\n",
      "Epoch 225/300 - DLoss: 0.666569 - GLoss: 32.870842\n",
      "Epoch 250/300 - DLoss: 0.637830 - GLoss: 37.715130\n",
      "Epoch 275/300 - DLoss: 0.608617 - GLoss: 35.518421\n",
      "Epoch 300/300 - DLoss: 0.585164 - GLoss: 34.029194\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "\n",
    "train_size = len(X)\n",
    "batch_size = 100\n",
    "\n",
    "model = GAN(784, 1, 100)\n",
    "model_name = 'mnist_feature_matching'\n",
    "\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        d_losses, g_losses = list(), list()\n",
    "        for _ in range(train_size//batch_size):\n",
    "            x_batch, y_batch = next_batch(X, y, batch_size)\n",
    "            un_batch = next_batch_(X_un, batch_size)\n",
    "            \n",
    "            feed_dict = dict()\n",
    "            feed_dict[model.X_lab] = x_batch\n",
    "            feed_dict[model.X_unl] = un_batch\n",
    "            feed_dict[model.y] = y_batch\n",
    "            feed_dict[model.eta] = 3e-3\n",
    "            feed_dict[model.is_training] = True\n",
    "            \n",
    "            d_loss, _ = sess.run([model.loss_disc, model.opt_disc], feed_dict)\n",
    "            g_loss, _ = sess.run([model.loss_gen, model.opt_gen], feed_dict)\n",
    "            d_losses.append(d_loss); g_losses.append(g_loss)\n",
    "            print(f'\\rEpoch {epoch+1:3}/{epochs:3} - DLoss: {d_loss:.6f} - GLoss: {g_loss:.6f}', end='')\n",
    "        print(f'\\rEpoch {epoch+1:3}/{epochs:3} - DLoss: {np.mean(d_losses):.6f} - GLoss: {np.mean(g_losses):.6f}', end='')\n",
    "        if (epoch+1) % 25 == 0:\n",
    "            print('')\n",
    "    \n",
    "    saver.save(sess, f'./models/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/mnist_feature_matching\n"
     ]
    }
   ],
   "source": [
    "model = GAN(784, 1, 100)\n",
    "model_name = 'mnist_feature_matching'\n",
    "\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, f'./models/{model_name}')\n",
    "    \n",
    "    feed_dict = dict()\n",
    "    feed_dict[model.X_lab] = dataset.test['images'][:10]\n",
    "    feed_dict[model.is_training] = False\n",
    "    \n",
    "    result = sess.run(model.predict, feed_dict)\n",
    "#     result = sess.run(model.generator, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 9, 7, 3, 7, 0, 3, 0, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(result, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f02846beda0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset.test['images'][1].reshape(-1, 28))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
