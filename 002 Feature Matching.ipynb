{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from mnist_dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    \n",
    "    def __init__(self, x_size, y_size, z_size):\n",
    "        self.x_size = x_size\n",
    "        self.y_size = y_size\n",
    "        self.z_size = z_size\n",
    "    \n",
    "        self.__build()\n",
    "    \n",
    "    def __build(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.__placeholders()\n",
    "            \n",
    "            # generator\n",
    "            z = tf.random_uniform((tf.shape(self.X_lab)[0], self.z_size))\n",
    "            g_model = self.__get_generator(z)\n",
    "            \n",
    "            # discriminators\n",
    "            d_lab, _ = self.__get_discriminator(self.X_lab)\n",
    "            d_unl, fn_1 = self.__get_discriminator(self.X_unl, True)\n",
    "            d_fake, fn_2 = self.__get_discriminator(g_model, True)\n",
    "            \n",
    "            # losses\n",
    "            ## Discriminator Loss\n",
    "            y_one_hot = tf.one_hot(self.y, 10)\n",
    "            loss_lab = tf.nn.softmax_cross_entropy_with_logits_v2(logits=d_lab, labels=y_one_hot)\n",
    "            loss_lab = tf.reduce_mean(loss_lab)\n",
    "            \n",
    "            l_unl = tf.reduce_logsumexp(d_unl, axis=1)\n",
    "            loss_unl = -.5*tf.reduce_mean(l_unl) + \\\n",
    "                        .5*tf.reduce_mean(tf.nn.softplus(l_unl)) + \\\n",
    "                        .5*tf.reduce_mean(tf.nn.softplus(d_fake))\n",
    "            \n",
    "            self.loss_disc = loss_lab + loss_unl\n",
    "            \n",
    "            ## Generator Loss\n",
    "            self.loss_gen = tf.losses.mean_squared_error(fn_1, fn_2)\n",
    "            \n",
    "#             loss_unl = -0.5*T.mean(l_unl) +\n",
    "#                  0.5*T.mean(T.nnet.softplus(nn.log_sum_exp(output_before_softmax_unl))) +\n",
    "#                 0.5*T.mean(T.nnet.softplus(nn.log_sum_exp(output_before_softmax_fake)))\n",
    "            \n",
    "            # optimizer\n",
    "            t_vars = tf.trainable_variables()\n",
    "            g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "            d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "            \n",
    "            self.opt_disc = tf.train.AdamOptimizer(self.eta, .5).minimize(self.loss_disc, var_list=d_vars)\n",
    "            self.opt_gen  = tf.train.AdamOptimizer(self.eta, .5).minimize(self.loss_gen, var_list=g_vars)\n",
    "        \n",
    "        self.predict = d_lab\n",
    "        self.generator = g_model\n",
    "    \n",
    "    def __placeholders(self):\n",
    "        self.X_lab = tf.placeholder(tf.float32, [None, self.x_size], 'X')\n",
    "        self.X_unl = tf.placeholder(tf.float32, [None, self.x_size], 'X_')\n",
    "        self.y = tf.placeholder(tf.int32, [None, self.y_size], 'y')\n",
    "        \n",
    "        self.eta = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        self.is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    \n",
    "    def __get_generator(self, tensor, reuse=False):\n",
    "        with tf.variable_scope('generator', reuse=reuse):\n",
    "            tensor = tf.layers.dense(tensor, 500, tf.nn.softplus)\n",
    "            tensor = tf.layers.batch_normalization(tensor, training=self.is_training)\n",
    "            \n",
    "            tensor = tf.layers.dense(tensor, 500, tf.nn.softplus)\n",
    "            tensor = tf.layers.batch_normalization(tensor, training=self.is_training)\n",
    "            \n",
    "            tensor = tf.layers.dense(tensor, self.x_size, tf.nn.sigmoid)\n",
    "        return tf.nn.l2_normalize(tensor, axis=1)\n",
    "    \n",
    "    def __get_discriminator(self, tensor, reuse=False):\n",
    "        with tf.variable_scope('discriminator', reuse=reuse):\n",
    "            tensor = self.__add_noise(tensor, .3)\n",
    "            \n",
    "            tensor = tf.layers.dense(tensor, 1000, tf.nn.relu)\n",
    "            tensor = self.__add_noise(tensor, .5)\n",
    "            \n",
    "            tensor = tf.layers.dense(tensor, 500, tf.nn.relu)\n",
    "            tensor = self.__add_noise(tensor, .5)\n",
    "            \n",
    "            tensor = tf.layers.dense(tensor, 250, tf.nn.relu)\n",
    "            tensor = self.__add_noise(tensor, .5)\n",
    "            \n",
    "            tensor = tf.layers.dense(tensor, 250, tf.nn.relu)\n",
    "            tensor = self.__add_noise(tensor, .5)\n",
    "            \n",
    "            tensor = fn = tf.layers.dense(tensor, 250, tf.nn.relu)\n",
    "            tensor = self.__add_noise(tensor, .5)\n",
    "            \n",
    "            tensor = tf.layers.dense(tensor, 10)\n",
    "        return tensor, fn\n",
    "    \n",
    "    def __add_noise(self, tensor, mu=0., std=1.):\n",
    "        noise = tf.random_normal(tf.shape(tensor), stddev=std)\n",
    "        return tensor + noise\n",
    "    \n",
    "model = GAN(784, 1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 1000\n",
    "ix = np.random.choice(range(42000), SAMPLE_SIZE)\n",
    "not_in_ix = [i for i in range(42000) if i not in ix]\n",
    "\n",
    "dataset = Dataset()\n",
    "X = dataset.train['images'][ix]\n",
    "y = dataset.train['labels'][ix]\n",
    "\n",
    "X_un = dataset.train['images'][not_in_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X, y, batch_size):\n",
    "    ix = np.random.choice(len(X), batch_size)\n",
    "    return X[ix], y[ix]\n",
    "\n",
    "def next_batch_(X, batch_size):\n",
    "    ix = np.random.choice(len(X), batch_size)\n",
    "    return X[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25/300 - DLoss: 1.217273 - GLoss: 8.873006\n",
      "Epoch  50/300 - DLoss: 0.961713 - GLoss: 13.457502\n",
      "Epoch  75/300 - DLoss: 0.890610 - GLoss: 15.710325\n",
      "Epoch 100/300 - DLoss: 0.779293 - GLoss: 19.365007\n",
      "Epoch 125/300 - DLoss: 0.787148 - GLoss: 18.215292\n",
      "Epoch 150/300 - DLoss: 0.606026 - GLoss: 22.949390\n",
      "Epoch 175/300 - DLoss: 0.670657 - GLoss: 31.916037\n",
      "Epoch 200/300 - DLoss: 0.573142 - GLoss: 27.654751\n",
      "Epoch 225/300 - DLoss: 0.591016 - GLoss: 28.410959\n",
      "Epoch 250/300 - DLoss: 0.535731 - GLoss: 33.266991\n",
      "Epoch 275/300 - DLoss: 0.533460 - GLoss: 28.405661\n",
      "Epoch 300/300 - DLoss: 0.452407 - GLoss: 34.167145\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "\n",
    "train_size = len(X)\n",
    "batch_size = 100\n",
    "\n",
    "model = GAN(784, 1, 100)\n",
    "model_name = 'mnist_feature_matching'\n",
    "\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        d_losses, g_losses = list(), list()\n",
    "        for _ in range(train_size//batch_size):\n",
    "            x_batch, y_batch = next_batch(X, y, batch_size)\n",
    "            un_batch = next_batch_(X_un, batch_size)\n",
    "            \n",
    "            feed_dict = dict()\n",
    "            feed_dict[model.X_lab] = x_batch\n",
    "            feed_dict[model.X_unl] = un_batch\n",
    "            feed_dict[model.y] = y_batch\n",
    "            feed_dict[model.eta] = 3e-3\n",
    "            feed_dict[model.is_training] = True\n",
    "            \n",
    "            d_loss, _ = sess.run([model.loss_disc, model.opt_disc], feed_dict)\n",
    "            g_loss, _ = sess.run([model.loss_gen, model.opt_gen], feed_dict)\n",
    "            d_losses.append(d_loss); g_losses.append(g_loss)\n",
    "            print(f'\\rEpoch {epoch+1:3}/{epochs:3} - DLoss: {d_loss:.6f} - GLoss: {g_loss:.6f}', end='')\n",
    "        print(f'\\rEpoch {epoch+1:3}/{epochs:3} - DLoss: {np.mean(d_losses):.6f} - GLoss: {np.mean(g_losses):.6f}', end='')\n",
    "        if (epoch+1) % 25 == 0:\n",
    "            print('')\n",
    "    \n",
    "    saver.save(sess, f'./models/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/mnist_feature_matching\n"
     ]
    }
   ],
   "source": [
    "model = GAN(784, 1, 100)\n",
    "model_name = 'mnist_feature_matching'\n",
    "\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, f'./models/{model_name}')\n",
    "    \n",
    "    feed_dict = dict()\n",
    "    feed_dict[model.X_lab] = dataset.test['images'][:10]\n",
    "    feed_dict[model.is_training] = False\n",
    "    \n",
    "    result = sess.run(model.predict, feed_dict)\n",
    "#     result = sess.run(model.generator, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 9, 7, 8, 7, 0, 3, 0, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(result, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1a8887d3c8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset.test['images'][9].reshape(-1, 28))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
